{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attribution Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtFlH0YEb8NPgZQvpM3J+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usamabaig1/Attribution_Modeling/blob/attributionModeller/Attribution_Modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnLCd0EoeaA2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "# %cd /gdrive/MyDrive/Criteo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH4NakHexGjt"
      },
      "source": [
        "import shutil, sys                                                                                                                                                    \n",
        "shutil.copy( '/gdrive/My Drive/Criteo/criteo_attribution_dataset300Camps.tsv', '/content/')\n",
        "# shutil.copy( '/gdrive/My Drive/Criteo/criteo_attribution_dataset.tsv', '/content/')\n",
        "# !cp -r 'criteo_attribution_dataset300Camps.tsv' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdrFs5SNxJsl"
      },
      "source": [
        "%pylab inline\n",
        "import pandas as pd\n",
        "plt.style.use('ggplot')\n",
        "from scipy.optimize import minimize\n",
        "from IPython.core.debugger import set_trace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGP4ngdCejsc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B60ts5nFewGp"
      },
      "source": [
        "# DATA_FILE='criteo_attribution_dataset.tsv.gz'\n",
        "# df = pd.read_csv(DATA_FILE, sep='\\t', compression='gzip')\n",
        "\n",
        "# DATA_FILE='criteo_attribution_dataset.tsv'\n",
        "DATA_FILE='criteo_attribution_dataset300Camps.tsv'\n",
        "df = pd.read_csv(DATA_FILE, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc2NTR4pfht_"
      },
      "source": [
        "# filteredCampaigns = df['campaign'].unique()[0:300] #selecting only 300 campaigns out of 675 (12515351 out of 16468027)\n",
        "# df = df[df['campaign'].isin(filteredCampaigns)]\n",
        "# df.to_csv('criteo_attribution_dataset300Camps.tsv', sep = '\\t')\n",
        "df.shape\n",
        "# !ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpDcouYufuie"
      },
      "source": [
        "\n",
        "\n",
        "  * **timestamp**: timestamp of the impression (starting from 0 for the first impression). The dataset is sorted according to timestamp.\n",
        "  *  **uid** a unique user identifier\n",
        "  * **campaign** a unique identifier for the campaign\n",
        "  * **conversion** 1 if there was a conversion in the 30 days after the impression (independently of whether this impression was last click or not)\n",
        "  * **conversion_timestamp** the timestamp of the conversion or -1 if no conversion was observed\n",
        "  *\t**conversion_id**\ta unique identifier for each conversion (so that timelines can be reconstructed if needed). -1 if there was no conversion\n",
        "  * **attribution** 1 if the conversion was attributed to Criteo, 0 otherwise\n",
        "  * **click** 1 if the impression was clicked, 0 otherwise\n",
        "  *\t**click_pos** the position of the click before a conversion (0 for first-click)\n",
        "  * **click_nb** number of clicks. More than 1 if there was several clicks before a conversion\n",
        "  * **cost** the price paid by Criteo for this display (**disclaimer:** not the real price, only a transformed version of it)\n",
        "  *\t **cpo** the cost-per-order  in case of attributed conversion (**disclaimer:** not the real price, only a transformed version of it)\n",
        "  * **time\\_since\\_last\\_click** the time since the last click (in s) for the given impression\n",
        "  *\t **cat[1-9]** contextual features associated to the display. Can be used to learn the click/conversion models. We do not disclose the meaning of these features but it is not relevant for this study. Each column is a categorical variable. In the experiments, they are mapped to a fixed dimensionality space using the Hashing Trick (see paper for reference)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OyKztJFe7xS"
      },
      "source": [
        "\n",
        "\n",
        "# 1. Data Exploration\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbW4O1Bmfc7K"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df.head()\n",
        "df.shape\n",
        "df.dtypes\n",
        "# df.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wAOw9hsfc4B"
      },
      "source": [
        "# df[(df['uid']== 7306395) & (df['campaign']== 29427842)]\n",
        "# a user can interact with the same campaign for more than one conversion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpYfg3I2hDM0"
      },
      "source": [
        "# df.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWyLijVafcyR"
      },
      "source": [
        "df['day'] = np.floor(df.timestamp / 86400.).astype(int)\n",
        "# df.head()\n",
        "# df[(df['uid']== 7306395) & (df['campaign']== 29427842)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoS8XIslfcrb"
      },
      "source": [
        "# df.click_pos.hist(bins = len(df.click_pos.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77UKpewOfchV"
      },
      "source": [
        "# df.click_nb.hist(bins = len(df.click_nb.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z1zmvHYfcMN"
      },
      "source": [
        "# df.day.hist(bins=len(df.day.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf6OuDggg61J"
      },
      "source": [
        "# 2. Data Manipulation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HaUl9pjfcJS"
      },
      "source": [
        "#the time diff between impression and conversion\n",
        "df['gap_click_sale'] = -1\n",
        "df.loc[df.conversion == 1, 'gap_click_sale'] = df.conversion_timestamp - df.timestamp\n",
        "# df['gap_click_sale_day'] = np.floor((df['gap_click_sale']/86400)).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKV4n4hCfcF1"
      },
      "source": [
        "# Attribution and the click on impression is the last click\n",
        "df['last_click'] = df.attribution * (df.click_pos == df.click_nb - 1).astype(int)\n",
        "# Attribution and the click on impression is the first click\n",
        "df['first_click'] = df.attribution * (df.click_pos == 0).astype(int)\n",
        "# Attribution only\n",
        "df['all_clicks'] = df.attribution\n",
        "# Attribution weighted by the weights\n",
        "df['uniform'] = df.attribution / (df.click_nb).astype(float)\n",
        "# df[(df['uid']== 7306395) & (df['campaign']== 29427842)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MDOrOHnfbq7"
      },
      "source": [
        "FEATURES = ['campaign', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', \n",
        "            'cat7', 'cat8']\n",
        "INFOS = ['cost', 'cpo', 'time_since_last_click', 'last_click', 'first_click', 'all_clicks', 'uniform']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNIao1TYtX6_"
      },
      "source": [
        "# 3. Attribution Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7UpkjIOfb-8"
      },
      "source": [
        "def bootstrap_method(data, sample_count, statistic, alpha):\n",
        "  \"\"\"Returns bootstrap estimate of sample_count*(1-alpha) CI for statistic.\"\"\"\n",
        "  # data = df['time_since_last_click']\n",
        "  # sample_count = 100\n",
        "  # statistic = np.mean\n",
        "  # alpha = 0.05\n",
        "  statitic_values = []\n",
        "  for x in range(0,sample_count):\n",
        "    sample = np.random.choice(np.array(data), len(np.array(data)), replace=True)\n",
        "    statistic_values.append(statitic(sample))\n",
        "  statitic_values = np.array(sorted(statitic_values))\n",
        "  return (statistic_values[int((alpha/2)*sample_count)], statistic_values[int((1-alpha/2)*sample_count)] )  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogFD6kjEfb1R"
      },
      "source": [
        "class weightedExp(nn.Module):\n",
        "  '''\n",
        "    A weighted exponential activation function\n",
        "  '''\n",
        "  def __init__(self, weight = 1e-3):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.tensor(weight))\n",
        "\n",
        "  def forward(self, input):\n",
        "    ex = torch.exp(-1*self.weight*input)\n",
        "    return (ex)\n",
        "\n",
        "\n",
        "def nllh_loss(input, pred, target, lambd):\n",
        "  '''\n",
        "    Implments NLLH loss as mentioned in the paper\n",
        "  '''\n",
        "  nllh = (target*input*lambd) - ((1-target)*torch.log(1 - pred))\n",
        "  nllhsum = nllh.sum()\n",
        "\n",
        "  return nllhsum / pred.data.nelement()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nnIMHq03W42"
      },
      "source": [
        "def optize_lambda(tts, attribution, verbose = False):\n",
        "  x_data = Variable(torch.Tensor(tts))\n",
        "  y_data = Variable(torch.Tensor(attribution))\n",
        "  #provided paramaters give closest approximation to value of lambda mentioned in the paper\n",
        "  learning_rate = 0.0001 \n",
        "  epochs = 10\n",
        "  model = weightedExp()\n",
        "  # criterion = nllh_loss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  for epoch in range(0,epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pred = model(x_data)\n",
        "    loss = nllh_loss(x_data, pred, y_data, model.weight)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if verbose:\n",
        "      print( \"Epoch:\", epoch)\n",
        "      print( \"Lambda:\", model.weight)\n",
        "  \n",
        "  return model.weight.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Vofr4Q6xd1R"
      },
      "source": [
        "df_view = df\n",
        "df_view.shape\n",
        "test_day =20\n",
        "learning_duration = 21\n",
        "df_train = df_view[(df_view.day >= test_day - learning_duration) & (df_view.day < test_day)]\n",
        "df_conv = df_train[df_train.click_pos == df_train.click_nb - 1] # selecting those impressions which were clicked last\n",
        "#only finding the chance of conversion given the time since last click\n",
        "x = df_conv.gap_click_sale.values\n",
        "y = df_conv.attribution.values \n",
        "\n",
        "lamb = optize_lambda(x, y)\n",
        "lamb\n",
        "# Lambda Value : 1.8000719137489796e-07"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zTkTdbvZP1"
      },
      "source": [
        "# AA attributions on full dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTftbMuOfbdn",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "def compute_aa_attributions(test_info, normalize=True):\n",
        "    test_info['idx'] = test_info.index\n",
        "    converted =  test_info[test_info.all_clicks==1]\n",
        "    # reconstructing the timelines for each conversion\n",
        "    conversion_ids = converted['conversion_id'].unique()\n",
        "    # reconstructing the timelines and applying attribution\n",
        "    by_conversion = converted[['conversion_id', 'timestamp', 'idx', 'bf_pred', 'time_since_last_click', 'last_click']].groupby('conversion_id')\n",
        "    new_clicks_data = []\n",
        "    \n",
        "    s_attr = []\n",
        "    s_attr_lc = []\n",
        "    # for each conversion compute attribution for each click\n",
        "    for conv, evts in by_conversion:\n",
        "        sorted_clicks = sorted(evts.values.tolist(), key=lambda x: x[1])\n",
        "        bf_pred = [_[3] for _ in sorted_clicks]\n",
        "        sum_bf = np.sum(bf_pred)\n",
        "        sum_lc = np.sum([_[5] for _ in sorted_clicks])\n",
        "        sum_attr = 0.0\n",
        "        for pos, (_, _, idx_, bf_, tslc_, lc_) in enumerate(sorted_clicks):\n",
        "            aa_attr = bf_pred[pos]\n",
        "            if normalize:\n",
        "                if sum_bf>0.0:\n",
        "                    aa_attr/=sum_bf\n",
        "                else:\n",
        "                    aa_attr = 0.0\n",
        "            sum_attr += aa_attr\n",
        "            new_clicks_data.append((idx_, aa_attr))\n",
        "        s_attr.append(sum_attr)\n",
        "        s_attr_lc.append(sum_lc)\n",
        "    \n",
        "    # now for each click, apply attribution from computed data\n",
        "    new_clicks_df = pd.DataFrame(columns=['click_idx', 'aa_attribution'])\n",
        "    cidx, attr = zip(*new_clicks_data)\n",
        "    new_clicks_df['click_idx'] = cidx\n",
        "    new_clicks_df['aa_attribution'] = attr\n",
        "    new_clicks_df = new_clicks_df.set_index('click_idx')\n",
        "    joined = test_info.join(new_clicks_df)\n",
        "    joined['aa_attribution'] = joined['aa_attribution'].fillna(value = 0.0)\n",
        "    return joined['aa_attribution']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7BAjlURe6fz"
      },
      "source": [
        "# compute the bid factor from aa attribution for each display\r\n",
        "gap_test = df.time_since_last_click.values\r\n",
        "previous_tslc_mask = (df.time_since_last_click >=0).astype(float)\r\n",
        "attr_pred = np.exp(-lamb*gap_test)\r\n",
        "attr_pred *= previous_tslc_mask\r\n",
        "bf_pred = 1 - attr_pred\r\n",
        "df['bf_pred'] = bf_pred\r\n",
        "df['AA_normed'] = compute_aa_attributions(df, normalize=True)\r\n",
        "df['AA_not_normed'] = compute_aa_attributions(df, normalize=False)\r\n",
        "INFOS += ['bf_pred', 'AA_normed', 'AA_not_normed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpCja_DkxTg8"
      },
      "source": [
        "Validation Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgqLcAwXe3Z-"
      },
      "source": [
        "def get_train_test_slice(df_view, test_day, learning_duration, label, features=None, \r\n",
        "                         hash_space=2**24, nrows=None, infos=None):\r\n",
        "    df_test = df_view[df_view.day == test_day]\r\n",
        "    if nrows is not None:\r\n",
        "        df_test = df_test[:nrows]\r\n",
        "    if features is None:\r\n",
        "        features = FEATURES\r\n",
        "    if infos is None:\r\n",
        "        infos = INFOS\r\n",
        "    df_train = df_view[(df_view.day >= test_day - learning_duration) & (df_view.day < test_day)]\r\n",
        "    if nrows is not None:\r\n",
        "        df_train = df_train[:nrows]\r\n",
        "  \r\n",
        "    X_train = df_train[features]\r\n",
        "    X_test = df_test[features]\r\n",
        "    \r\n",
        "    hasher = FeatureHasher(n_features=hash_space, non_negative=1)\r\n",
        "    \r\n",
        "    def to_dict_values(df_view):\r\n",
        "        return [dict([(_[0]+str(_[1]),1) for _ in zip(features,l)]) for l in df_view.values]\r\n",
        "    \r\n",
        "    X_train_h = hasher.fit_transform(to_dict_values(X_train))\r\n",
        "    X_test_h = hasher.transform(to_dict_values(X_test))\r\n",
        "    y_train = df_train[label]\r\n",
        "    y_test = df_test[label]\r\n",
        "    return (X_train_h, y_train), (X_test_h, y_test), df_test[infos], df_train.last_click.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7XSe92hxY6V"
      },
      "source": [
        "Compute Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAASbRJd78IV"
      },
      "source": [
        "from scipy.special import gammainc\r\n",
        "def empirical_utility(a, v, c, p):\r\n",
        "    won = np.array(p*v > c, dtype=np.int)\r\n",
        "    return (a*v)*won, -c*won\r\n",
        "\r\n",
        "def expected_utility(a, v, c, p, beta=1000):\r\n",
        "    return a*v*gammainc(beta*c+1, beta*p*v) - ((beta*c+1)/beta)*gammainc(beta*c+2, beta*p*v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21DNU80Lxkfq"
      },
      "source": [
        "def evaluate_utility(y_pred, utilities, betas, test_info):\r\n",
        "    partial_score = dict()\r\n",
        "    for utility in utilities:\r\n",
        "        attribution = test_info[utility]\r\n",
        "        for beta in betas:\r\n",
        "            if np.isinf(beta):\r\n",
        "                est_utility = empirical_utility(attribution, test_info.cpo, test_info.cost, y_pred)\r\n",
        "            else:\r\n",
        "                est_utility = expected_utility(attribution, test_info.cpo, test_info.cost, y_pred, beta=beta)\r\n",
        "            beta_str = str(beta) if not np.isinf(beta) else 'inf'\r\n",
        "            partial_score['utility-'+utility+'-beta'+beta_str] = est_utility\r\n",
        "    return partial_score\r\n",
        "\r\n",
        "def get_naive_baseline(y_train, X_test):\r\n",
        "    return np.mean(y_train)*np.ones(X_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwiqklWuxpoF"
      },
      "source": [
        "def evaluate_day_for_bidder(df_view, test_day, learning_duration, bidder, utilities, betas,\r\n",
        "                            hash_space=None, features=None, clf=None, AA_bidder_label=None, recalibrate=True):\r\n",
        "    score = dict()\r\n",
        "    bid_profile = dict()\r\n",
        "    label = bidder\r\n",
        "    if bidder == 'AA':\r\n",
        "        label = AA_bidder_label\r\n",
        "    # get data slice\r\n",
        "    (X_train, y_train), (X_test, y_test), test_info, y_train_lc_mean = get_train_test_slice(df_view,\r\n",
        "                                                                           test_day,\r\n",
        "                                                                           learning_duration,\r\n",
        "                                                                           label=label, \r\n",
        "                                                                           hash_space = hash_space,\r\n",
        "                                                                           features=features)           \r\n",
        "    \r\n",
        "    # learn the model\r\n",
        "    clf.fit(X_train, y_train)\r\n",
        "    \r\n",
        "    # get test predictions\r\n",
        "    y_pred = clf.predict_proba(X_test)[:,1]            \r\n",
        "    \r\n",
        "    # if aa bidder: modulate the bids by bid_factor computed from attribution model\r\n",
        "    if bidder == 'AA':\r\n",
        "        y_pred *= test_info['bf_pred']\r\n",
        "    \r\n",
        "    # compute the loss\r\n",
        "    loss = log_loss(y_test, y_pred, normalize=0)\r\n",
        "    \r\n",
        "    # loss of baseline model\r\n",
        "    baseline_loss = log_loss(y_test, get_naive_baseline(y_train, X_test), normalize=0)\r\n",
        "    score['nllh'] = loss\r\n",
        "    score['nllh_naive'] = baseline_loss\r\n",
        "    \r\n",
        "    # do we recalibrate output? (i.e recalibrate mean prediction). This is usually done by a control system.\r\n",
        "    if recalibrate:\r\n",
        "        y_pred *= (y_train_lc_mean / y_pred.mean())\r\n",
        "    \r\n",
        "    #how many displays are won?\r\n",
        "    won = (y_pred*test_info.cpo > test_info.cost).astype(int)\r\n",
        "    score['won'] = np.sum(won)\r\n",
        "    score['n_auctions'] = y_pred.shape[0]\r\n",
        "    \r\n",
        "    # compute the scores on this slice\r\n",
        "    score.update(evaluate_utility(y_pred, utilities, betas, test_info))\r\n",
        "    \r\n",
        "    #store bid profiles\r\n",
        "    bid_profile['time_since_last_click'] = test_info.time_since_last_click\r\n",
        "    bid_profile['bid'] = y_pred\r\n",
        "    \r\n",
        "    return score, bid_profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9TaFNBxxuJ8"
      },
      "source": [
        "def merge_utility_score(score):\r\n",
        "    updates = dict()\r\n",
        "    for k,v in score.items():\r\n",
        "        if not 'utility' in k:\r\n",
        "            continue\r\n",
        "        if 'inf' in k:\r\n",
        "            revenue, cost = v\r\n",
        "            updates[k] = np.sum(cost) + np.sum(revenue)\r\n",
        "            updates[k+'~revenue'] = np.sum(revenue)\r\n",
        "            updates[k+'~cost'] = np.sum(cost)\r\n",
        "            v = revenue + cost\r\n",
        "        else:\r\n",
        "            updates[k] = np.sum(v)\r\n",
        "        bounds = bootstrap(v, 100, np.sum, .05)\r\n",
        "        delta = (bounds[1]-bounds[0])/2.\r\n",
        "        updates[k+'-delta'] = delta\r\n",
        "    score.update(updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_YsyLPuxvPg"
      },
      "source": [
        "def update_score(partial_score, score):\r\n",
        "    for k, v in partial_score.items():\r\n",
        "        if 'utility' in k:\r\n",
        "            if 'inf' in k:\r\n",
        "                revenue, cost = v\r\n",
        "                print('\\t\\t', k, np.sum(cost)+np.sum(revenue))\r\n",
        "                current_revenue, current_cost = score.get(k, (np.array([]),np.array([])))\r\n",
        "                score[k] = (\r\n",
        "                    np.append(current_revenue, revenue),\r\n",
        "                    np.append(current_cost, cost)\r\n",
        "                )\r\n",
        "            else:\r\n",
        "                print('\\t\\t', k, np.sum(v))\r\n",
        "                score[k] = np.append(score.get(k, np.array([])), v)\r\n",
        "        else:\r\n",
        "            print('\\t\\t', k, v)\r\n",
        "            score[k] = score.get(k, 0) + v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Sp8h59xyw0"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwyttgGrx2dW"
      },
      "source": [
        "from datetime import datetime, timedelta\r\n",
        "def evaluate_slices(df_view,\r\n",
        "                    bidders=['last_click', 'first_click', 'AA'],\r\n",
        "                    utilities=['last_click','first_click', 'AA_normed', 'AA_not_normed'],\r\n",
        "                    betas=[np.inf, 10, 1000],\r\n",
        "                    test_days=[22],\r\n",
        "                    learning_duration=21,\r\n",
        "                    hash_space=2**24,\r\n",
        "                    features=None,\r\n",
        "                    AA_bidder_label='all_clicks',\r\n",
        "                    clf = LogisticRegression(solver='lbfgs', n_jobs=4),\r\n",
        "                    recalibrate = True):\r\n",
        "    bid_profiles = []\r\n",
        "    scores = []\r\n",
        "    for bidder in bidders:\r\n",
        "        print ('*'*80)\r\n",
        "        print(\"EVALUATING BIDDER:\", bidder)\r\n",
        "        score = dict()\r\n",
        "        bid_profile = dict()\r\n",
        "        for test_day in test_days:\r\n",
        "            start = datetime.now()\r\n",
        "            print('\\t- day:', test_day)\r\n",
        "            partial_score, partial_bid_profile = evaluate_day_for_bidder(\r\n",
        "                df_view, test_day, learning_duration, bidder, \r\n",
        "                utilities, betas,\r\n",
        "                hash_space=hash_space, features=features, clf=clf, \r\n",
        "                AA_bidder_label=AA_bidder_label, recalibrate=recalibrate\r\n",
        "            )\r\n",
        "            update_score(partial_score, score)\r\n",
        "            for k, v in partial_bid_profile.items():\r\n",
        "                bid_profile[k] = np.append(bid_profile.get(k, np.array([])), v)\r\n",
        "            print('\\t- took', datetime.now() - start)\r\n",
        "        score['bidder'] = bidder\r\n",
        "        bid_profile['bidder'] = bidder\r\n",
        "        score['nllh_comp_vn'] = (score['nllh_naive'] - score['nllh']) / np.abs(score['nllh_naive'])\r\n",
        "        score['win_rate'] = score['won'] / score['n_auctions']\r\n",
        "        merge_utility_score(score)\r\n",
        "        scores.append(score)\r\n",
        "        bid_profiles.append(bid_profile)\r\n",
        "    return pd.DataFrame(scores), pd.DataFrame(bid_profiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLW3-NwTywgK"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyYQa_IayyYK"
      },
      "source": [
        "#full run\r\n",
        "if False:\r\n",
        "    scores, bid_profiles = evaluate_slices(df,\r\n",
        "                                       bidders=['last_click',\r\n",
        "                                                'first_click',\r\n",
        "                                                'AA'],\r\n",
        "                                       utilities=['last_click',\r\n",
        "                                                  'first_click',\r\n",
        "                                                  'AA_normed',\r\n",
        "                                                  'AA_not_normed'],\r\n",
        "                                       test_days=range(22,29),\r\n",
        "                                       learning_duration=21,\r\n",
        "                                       hash_space = 2**18,\r\n",
        "                                       AA_bidder_label='all_clicks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibOUTFrcy1FA"
      },
      "source": [
        "#simple debug run\r\n",
        "if True:\r\n",
        "    scores, bid_profiles = evaluate_slices(df,\r\n",
        "                                       bidders=['last_click',\r\n",
        "                                                'AA'],\r\n",
        "                                       utilities=['last_click',\r\n",
        "                                                  'AA_normed'],\r\n",
        "                                       test_days=range(22,23),\r\n",
        "                                       learning_duration=5,\r\n",
        "                                       hash_space = 2**13,\r\n",
        "                                       AA_bidder_label='all_clicks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unIc6rhEy2GU"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}